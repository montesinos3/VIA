{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios Entrega 4\n",
    "\n",
    "## SIFT\n",
    "### Añade al ejercicio CLASIFICADOR un método basado en el número de coincidencias de keypoints SIFT. Utilízalo para reconocer objetos con bastante textura (p. ej. carátulas de CD, portadas de libros, cuadros de pintores, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejercicio, en primer lugar he implementado el método SIFT con las funciones `precompute()` y `compare()`. \n",
    "\n",
    "Para ello, he modificado el fichero `sift.py` del directorio `code/SIFT` de modo que para la función `precompute()` obtuviera todos los keypoints del modelo que le paso como parámetro, que como se explicó anteriormente, esta función se ejecuta para todos los modelos de el directorio pasada como argumento. \n",
    "\n",
    "Posteriormente, he implementado la función `compare()`, la cual dada el frame y los keypoints de un modelo, devuelve cuantos de ellos son comunes, de modo que cuando se ejecute el compare para cada uno de los modelos, podamos obtener el que mayor coincidencia tiene con el frame actual.\n",
    "\n",
    "He de decir también que he creado el directorio images en la ruta `Entregas/Entrega3/src/images` para que al ejecutar se guarden los modelos nuevos allí. \n",
    "\n",
    "De modo que la ejecución ahora se haría desde el directorio `Entrega3/src/` de la siguiente manera:\n",
    "\n",
    "`python main.py --models=./images --method=sift --save` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta aquí todo nos es familiar puesto que es muy similar a la entrega anterior. Sin embargo, ahora en el main hay que tratar de representar esas conincidencias de manera visual.\n",
    "\n",
    "Para ello en el fichero `main.py` he modificado la función `main()` de modo que si el método actual es el sift, una vez comprobado cual es el modelo con mejor coincidencia, calcule los keypoints del frame actual y los asocie con los de dicho modelo para poder representarlos en el frame.\n",
    "\n",
    "Esto se consigue de la siguiente manera:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # ... \n",
    "    # Hasta aquí el codigo no cambia\n",
    "\n",
    "    # Iniciar captura de video\n",
    "    for key, frame in autoStream():\n",
    "\n",
    "        # ... \n",
    "        # Hasta aquí el codigo no cambia\n",
    "        \n",
    "        if args.method != \"sift\":\n",
    "            # Mostramos el frame de la misma manera que se hace en la entrega 3\n",
    "        else: \n",
    "            sift = cv2.SIFT_create(nfeatures=500)\n",
    "            mejor_imagen=models[best_match[0]]['image']\n",
    "            t0 = time.time()\n",
    "            keypoints , descriptors = sift.detectAndCompute(result_frame, mask=None)\n",
    "            t1 = time.time()\n",
    "            putText(result_frame, f'{len(keypoints)} pts  {1000*(t1-t0):.0f} ms')\n",
    "            \n",
    "            # Recuperar keypoints y descriptores del modelo con mejor coincidencia\n",
    "            k0,d0 = models[best_match[0]]['features']['keypoints'], models[best_match[0]]['features']['descriptors']\n",
    "            x0=mejor_imagen\n",
    "            t2 = time.time()\n",
    "            matches = cv2.BFMatcher().knnMatch(descriptors, d0, k=2)\n",
    "            t3 = time.time()\n",
    "            # Aplicar el \"ratio test\" para filtrar coincidencias\n",
    "            good = []\n",
    "            for m in matches:\n",
    "                if len(m) >= 2:\n",
    "                    best, second = m\n",
    "                    if best.distance < 0.75 * second.distance:\n",
    "                        good.append(best)\n",
    "\n",
    "            if len(good) > 0:\n",
    "                imgm = cv2.drawMatches(result_frame, keypoints, x0, k0, good,\n",
    "                                    flags=0,\n",
    "                                    matchColor=(128,255,128),\n",
    "                                    singlePointColor=(128,128,128),\n",
    "                                    outImg=None)\n",
    "            else:\n",
    "                print(\"No hay coincidencias buenas para dibujar.\")\n",
    "                imgm = result_frame.copy()\n",
    "            putText(imgm ,f'{len(good)} matches  {1000*(t3-t2):.0f} ms', \n",
    "                      orig=(5,36), color=(200,255,200))   \n",
    "            cv2.imshow('Result', imgm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado de todo ello es lo que se observa en las siguientes imágenes:\n",
    "\n",
    "![alt text](images/portadaAlbum1.png)\n",
    "\n",
    "![alt text](images/portadaAlbum2.png)\n",
    "\n",
    "Donde, como se puede apreciar se calculan los keypoints para el frame actual y se asocian con los keypoints de cada una de las imágenes gracias a los descriptores de ambos frames, calculados gracias a la función `detectAndCompute()` y asociados mediante la función `knnMatch()` del matcher de OpenCV `cv2.BFMatcher()`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
